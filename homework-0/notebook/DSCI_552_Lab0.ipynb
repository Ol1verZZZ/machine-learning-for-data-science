{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><h1>DSCI-552 Lab 0</h1></center>\n",
    "<br>\n",
    "<center><font size=\"4\">Introduction to Basic Development Tools</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Please read the instructions and problem prompts **carefully**.\n",
    "2. This lab is to give you some basic APIs of numpy, pandas and scikit-learn. Besides, some topics such as how to make your jupyter notebook be a more efficient developing tools, how to use git and GitHub will also be covered. The lab is to be done individually. You may talk to your fellow classmates about general issues (\"Remind me again: Which API should I used for doing group by operation to a data set\") but about the specifies of how to do these exercises.\n",
    "3. Along with a similar vein, you can ask the TA for help, but ask questions about **concepts** but not ask the TA to help you debug your code. The TA is here to help, but not to do the work for you.\n",
    "4. You are welcome to use the class resources and the Internet.\n",
    "5. Playing with variations. Solve one problems, and then copy the code to a new cell and play around with it. Doing this is the single most important thing when learning programming.\n",
    "6. This lab will not be graded but the content is highly related to your future programming assignments. So, treat it wisely.\n",
    "7. All the content having been gone though in the week 1 discussion is just a snapshot of the most basic concepts. **You need to keep study more about Git, GitHub, Pandas, Numpy and Scikit-Learn in order to finish your programming assignments successfully.**\n",
    "8. Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Development Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to setup the environment. But, I do recommend a simple idea that is using the Anaconda, which is a pre-build python environment with bundles of useful packages.\n",
    "\n",
    "**To download the Anaconda, go to the following website:\n",
    "https://www.anaconda.com/distribution/**. Download the correct version based on your operating system and install it step by step.\n",
    "\n",
    "Then, **configure your PATH environment variable** to make the conda command work. The following command is an easy way to test whether your configuration is correct. If it is, you will see something as like as the sample output.\n",
    "\n",
    "> **command:**\n",
    ">\n",
    "> conda --version\n",
    ">\n",
    "> **sample output:**\n",
    ">\n",
    "> conda 4.6.12\n",
    "\n",
    "**Finally, download this jupyter notebook file,** then change the working directory to where its location in terminal, and type the following command to open the jupter notebook and finish the lab.\n",
    "\n",
    "> **command:** \n",
    "> jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T19:44:42.243800Z",
     "start_time": "2020-01-14T19:44:41.609555Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The read_csv() Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T03:51:17.953542Z",
     "start_time": "2019-08-28T03:51:17.937799Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Salaries.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7fd0232f1462>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Salaries.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'playerID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# pd.read_csv('Salaries.csv', header=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# pd.read_csv('Salaries.csv', skiprows=[2], header=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Salaries.csv'"
     ]
    }
   ],
   "source": [
    "pd.read_csv('Salaries.csv',index_col='playerID')\n",
    "# pd.read_csv('Salaries.csv', header=1)\n",
    "# pd.read_csv('Salaries.csv', skiprows=[2], header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing and Selecting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the id of the players who are registered in ATL and HOU and whose salary is higher than one million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T03:51:24.015061Z",
     "start_time": "2019-08-28T03:51:23.982621Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Salaries.csv',index_col='playerID')\n",
    "greater_than_1_mil = data[((data.teamID=='ATL') | (data.teamID=='HOU')) & (data.salary>=1000000)]\n",
    "print ('Number of players with ALT or HOU having salary greater than 1 million : ', greater_than_1_mil.shape[0])\n",
    "print (greater_than_1_mil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The describe() Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the standard Deviation, first quartile, medium, third quartile, mean, maximum, minimum of the salary in team ATL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T01:46:27.395276Z",
     "start_time": "2019-08-28T01:46:27.368849Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Salaries.csv',index_col='playerID')\n",
    "data_ATL = data[(data.teamID=='ATL')]\n",
    "data_ATL_std = data_ATL.std(axis=0)\n",
    "print('Standard Deviation of team ATL salary is : ', data_ATL_std.salary)\n",
    "data_ATL_mean = data_ATL.mean(axis=0)\n",
    "print('Mean of team ATL salary is : ', data_ATL_mean.salary)\n",
    "data_ATL_max = data_ATL.max(axis=0)\n",
    "print('Max of team ATL salary is : ', data_ATL_max.salary)\n",
    "data_ATL_quartile = data_ATL.quantile([0.25, 0.75, 0.5])\n",
    "print('First and Third Quartiles, medium of team ATL salary is : ', data_ATL_quartile.salary)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The iterrows() Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Python dictionary object whose keys are the headers of the dataframe created in the read_csv() exercise and values are Python list objects that contain data corresponding to the headers. (Here, use the iterrows method to iterate each row of the dataframe and copy it to a dictionary. However, there is a easier way. Learn how the to_dict() method works by yourself later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T03:51:38.476575Z",
     "start_time": "2019-08-28T03:51:36.568044Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 1st way:\n",
    "# import pandas as pd\n",
    "# data = pd.read_csv('Salaries.csv')\n",
    "# df = pd.DataFrame(data)\n",
    "# case_list = []\n",
    "# for index, row in df.iterrows():\n",
    "#     my_dict = {index : row}\n",
    "#     case_list.append(my_dict)\n",
    "# print(case_list)\n",
    "\n",
    "# # 2nd way:\n",
    "import pandas as pd\n",
    "data = pd.read_csv('Salaries.csv')\n",
    "# df = pd.DataFrame(data)\n",
    "result = data.to_dict(orient='records')\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataframe Using the Constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T15:53:45.113577Z",
     "start_time": "2019-08-24T15:53:45.110554Z"
    }
   },
   "source": [
    "Read the documentation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame and create a dataframe using pd.DataFrame from the dictionary created in the iterrows() exercise. Change the header to \"a\", \"b\", \"c\", ... at creation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T03:53:06.129037Z",
     "start_time": "2019-08-28T03:53:06.074438Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result)\n",
    "df.columns = ['a', 'b', 'c', 'd', 'e']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick start: https://www.numpy.org/devdocs/user/quickstart.html\n",
    "\n",
    "Numpy axes explaination: https://www.sharpsightlabs.com/blog/numpy-axes-explained/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The np.array Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Example 1:\n",
    "\n",
    "```python\n",
    "ls = [1, 2, 3]\n",
    "arr = np.array(ls)\n",
    "```\n",
    "\n",
    "Example 2:\n",
    "```python\n",
    ">>> np.array([[1, 2], [3, 4]])\n",
    "array([[1, 2],\n",
    "       [3, 4]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a 2-dimensional Python list object, then convert it to a Numpy array object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ndarray Objects' Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with the **ndim, shape, size, dtype, itemsize and data** attribute.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    ">>> arr = np.array([[1, 2], [3, 4]])\n",
    ">>> arr.ndim\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[1, 2], [3, 4]])\n",
    "arr.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension of ndarray Ojects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with the reshape() and flatten() method.\n",
    "\n",
    "Example:\n",
    "```python\n",
    ">>> arr = np.array([[1, 2], [3, 4]])\n",
    ">>> arr.flatten()\n",
    "array([1, 2, 3, 4])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[1, 2], [3, 4]])\n",
    "arr.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Slice Operation of ndarray Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand how the slice operation works for 1-D array and 2-D array.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    ">>> arr = np.array([[1, 2, 3], [3, 4, 6], [7, 8, 9]])\n",
    ">>> arr[1:]\n",
    "array([[3, 4, 6],\n",
    "       [7, 8, 9]])\n",
    ">>> arr[1:, 0:2]\n",
    "array([[3, 4],\n",
    "       [7, 8]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[1, 2, 3], [3, 4, 6], [7, 8, 9]])\n",
    "print(arr[1:])\n",
    "arr[1:, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Calculation of ndarray Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with the **argmin(), argmax(), min(), max(), mean(), sum(), std(), dot(), square(), sqrt(), abs(). exp(), sign(), mod()** method.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    ">>> np.square(array)\n",
    "array([[ 1,  4,  9],\n",
    "       [ 9, 16, 36],\n",
    "       [49, 64, 81]])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Important Methods Inside Module Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with the arange(), ones(), zeros(), eye(), linspace(), concatenate() method.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    ">>> np.eye(3)\n",
    "array([[1., 0., 0.],\n",
    "       [0., 1., 0.],\n",
    "       [0., 0., 1.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.eye(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followings are packages (or methods) in Python (Scikit-Learn and Scipy) that will be frequently used in your programming assignment. So, please read carefully.\n",
    "\n",
    "- Data Preprocessing (https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)\n",
    "    - Standardization: StandardScaler\n",
    "    - Normalization: MinMaxScaler\n",
    "    - Quantifing Categorical Features: LabelEncoder. OneHotEncoder\n",
    "    - Construct Train and Test Set: model_selection.train_test_split\n",
    "- KNN: KNeighborsClassifier\n",
    "- Linear Regression: LinearRegression\n",
    "- Logistic Regression: LogisticRegression, LogisticRegressionCV\n",
    "- Feature Selection / Model Selection\n",
    "    - L1 Penalized Regression (Lasso Regression) with Cross-Validation: LassoCV\n",
    "    - L2 Penalized Regression (Ridge Regression) with Cross-Validation: RidgeCV\n",
    "    - Cross-Validation: StratifiedKFold, RepeatedKFold, LeaveOneOut, KFold, model_selection.cross_validate, model_selection.cross_val_predict, model_selection.cross_val_score\n",
    "    - Model Metrics (https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics): accuracy_score, auc, f1_score, hamming_loss, precision_score, recall_score, roc_auc_score\n",
    "- Decision Tree: DecisionTreeClassifier, DecisionTreeRegressor\n",
    "- Bootstrap, Ensemble Methods\n",
    "    - Bootstrap: bootstrapped (https://pypi.org/project/bootstrapped/)\n",
    "    - Bagging: RandomForestClassifier, RandomForestRegressor\n",
    "    - Boosting: AdaBoostClassifier, AdaBoostRegressor\n",
    "- Support Vector Machines (https://scikit-learn.org/stable/modules/svm.html#svm): LinearSVC, LinearSVR\n",
    "- Multiclass and Multilabel Classification (https://scikit-learn.org/stable/modules/classes.html#module-sklearn.multiclass)\n",
    "    - One-vs-one Multiclass Strategy: OneVsOneClassifier\n",
    "    - One-vs-the-rest (OvR) multiclass/multilabel strategy / OneVsRestClassifier\n",
    "- Unsupervised Learning\n",
    "    - K-means Clustering: KMeans\n",
    "    - Hierarchical Clustering: scipy.cluster.hierarchy (not scikit-learn)\n",
    "- Semisupervised Learning (https://scikit-learn.org/stable/modules/label_propagation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick start:** https://matplotlib.org/3.1.1/tutorials/introductory/pyplot.html\n",
    "\n",
    "**Exercises:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Create two one dimensional arrays x and y and plot y vs x, add title, xlabel, ylabel, grid.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, num=20)\n",
    "y = np.array([j ** 2 for j in x])\n",
    "```\n",
    "\n",
    "copy the code above to the following cell and add code for plotting the parabola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T10:38:20.467433Z",
     "start_time": "2020-01-08T10:38:20.462120Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-5, 5, num=20)\n",
    "y = np.array([j ** 2 for j in x])\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T10:43:28.333944Z",
     "start_time": "2020-01-08T10:43:28.260989Z"
    }
   },
   "source": [
    "What happens if the independent variable is not sorted before plotting? Try plotting directly using the following defined array.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, num=20)\n",
    "np.random.shuffle(x)\n",
    "y = np.array([j ** 2 for j in x])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, num=20)\n",
    "np.random.shuffle(x)\n",
    "y = np.array([j ** 2 for j in x])\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Create multiple arrays and plot them with different styles, add legends, add text/mathematical equations on the plot.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, num=20)\n",
    "y1 = np.array([j for j in x])\n",
    "y2 = np.array([j ** 2 for j in x])\n",
    "y3 = np.array([j ** 3 for j in x])\n",
    "```\n",
    "\n",
    "copy the code above to the following cell and add code for plotting curve $\\left(x, y1\\right)$, $\\left(x, y2\\right)$ and $\\left(x, y3\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, num=20)\n",
    "y1 = np.array([j for j in x])\n",
    "y2 = np.array([j ** 2 for j in x])\n",
    "y3 = np.array([j ** 3 for j in x])\n",
    "\n",
    "plt.plot(x, y1)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y1')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, y2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y2')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, y3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Create multiple arrays and plot them into one figure **(No multiple figure and no subplot is allowed in this question)**.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, num=20)\n",
    "y1 = np.array([j for j in x])\n",
    "y2 = np.array([j ** 2 for j in x])\n",
    "y3 = np.array([j ** 3 for j in x])\n",
    "```\n",
    "\n",
    "copy the code above to the following cell and add code for plotting curve $\\left(x, y1\\right)$, $\\left(x, y2\\right)$ and $\\left(x, y3\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, num=20)\n",
    "y1 = np.array([j for j in x])\n",
    "y2 = np.array([j ** 2 for j in x])\n",
    "y3 = np.array([j ** 2 for j in x])\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
    "ax1.plot(x, y1)\n",
    "ax2.plot(x, y2)\n",
    "ax3.plot(x, y3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Create multiple subplots, play around with the figure size, figure title, and its font style and font size **(One curve is plotted in one subplot in this question)**.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, num=20)\n",
    "y1 = np.array([j for j in x])\n",
    "y2 = np.array([j ** 2 for j in x])\n",
    "y3 = np.array([j ** 3 for j in x])\n",
    "```\n",
    "\n",
    "copy the code above to the following cell and add code for plotting curve $\\left(x, y1\\right)$, $\\left(x, y2\\right)$ and $\\left(x, y3\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, num=20)\n",
    "y1 = np.array([j for j in x])\n",
    "y2 = np.array([j ** 2 for j in x])\n",
    "y3 = np.array([j ** 2 for j in x])\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, figsize=(9.5, 5.5))\n",
    "fig.suptitle('Vertically stacked subplots')\n",
    "\n",
    "csfont = {'fontname':'Comic Sans MS'}\n",
    "hfont = {'fontname':'Helvetica'}\n",
    "\n",
    "plt.title('Vertically stacked subplots',**csfont)\n",
    "plt.xlabel('x', **hfont)\n",
    "\n",
    "ax1.plot(x, y1)\n",
    "ax2.plot(x, y2)\n",
    "ax3.plot(x, y3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Change the limits on x and y axes, **use logarithmic axes to plot**.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, num=20)\n",
    "y = np.array([j ** 2 for j in x])\n",
    "```\n",
    "\n",
    "copy the code above to the following cell and add code for plotting the parabola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, num=20)\n",
    "y = np.array([j for j in x])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, y)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.xlabel('logx')\n",
    "plt.ylabel('logy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas's DataFrame.plot and Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas's DataFrame.plot\n",
    "\n",
    "Use the Salaries.csv again (You can use the dataframe object loaded from section 3.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) For team 'ATL', plot a scatter plot between feature yearID and salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Salaries.csv',index_col='playerID')\n",
    "df = pd.DataFrame(data)\n",
    "df_copy = df\n",
    "cols_with_team_ATL = df_copy.loc[df_copy.teamID==\"ATL\", ]\n",
    "\n",
    "cols_with_team_ATL.plot.scatter(x = 'salary', y = 'yearID', title=\"Scatterplot between yearID and salary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) For year 1985, plot a bar chart to show the average salary for each team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_year_1985 = df_copy.loc[df_copy.yearID==\"1985\", ]\n",
    "cols_with_year_1985['salary'] = cols_with_year_1985['salary'].astype(int)\n",
    "\n",
    "cols_with_year_1985.plot.bar(x=\"teamID\", y=\"salary\", rot=70, title=\"Average Salary for each team for the year 1985\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) For team 'ATL', plot a line chart to show how the annual average salary change by years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_team_ATL['salary'] = cols_with_team_ATL['salary'].astype(int)\n",
    "cols_with_team_ATL['yearID'] = cols_with_team_ATL['yearID'].astype(int)\n",
    "\n",
    "cols_with_team_ATL.plot.line(x = 'yearID', y = 'salary', title=\"Average Salary change by year for the team = ATL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Append one more numeric feature to the data frame (can be generated randomly), then for team 'ATL', use the seaborn.pairplot to plot scatter plots among all numeric features in the data frame for team. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "var_set = [\n",
    "    \"yearID\",\n",
    "    \"teamID\",\n",
    "    \"lgID\",\n",
    "    \"playerID\",\n",
    "    \"salary\"\n",
    "]\n",
    "\n",
    "head_set = []\n",
    "head_set.extend(var_set)\n",
    "head_set.append(\"num_feat\")\n",
    "\n",
    "df = pd.read_csv('Salaries.csv',index_col='playerID', header=None, names=head_set)\n",
    "\n",
    "df['num_feat'] = 100 * np.random.random_sample(df.shape[0])\n",
    "\n",
    "df_copy = df\n",
    "cols_with_team_ATL = df_copy.loc[df_copy.teamID==\"ATL\", ]\n",
    "\n",
    "# Create the default pairplot\n",
    "pairplot_fig = sns.pairplot(cols_with_team_ATL, vars=['yearID', 'salary', 'num_feat'])\n",
    "plt.subplots_adjust(top=0.9)\n",
    "pairplot_fig.fig.suptitle(\"Scatter plots among all numeric features in the data frame for teamID = ATL\", fontsize=18, alpha=0.9, weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) For year 1985 and for each team, plot a boxplot to show how the salary distribute within a team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_year_1985 = df_copy.loc[df_copy.yearID==\"1985\", ]\n",
    "cols_with_year_1985['salary'] = cols_with_year_1985['salary'].astype(int)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "sns.boxplot(x='teamID', y='salary', data=cols_with_year_1985, orient='v')\n",
    "fig.subplots_adjust(top=0.9)\n",
    "ax.text(x=0.5, y=1.1, s=\"Boxplot of salary distribution for the year 1985 for each team\", fontsize=16, weight='bold', ha='center', va='bottom', transform=ax.transAxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Read the offical documentation (https://seaborn.pydata.org/) to understand how lmplot, catplot, relplot, and jointplot works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jupyter Notebook Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensions such as the code formatter, table of content is to make your development more efficient. To explore it, please refer to https://github.com/ipython-contrib/jupyter_contrib_nbextensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jupyter Visual Debugger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pixie Debugger is a visual debugger for debugging on Jupyter Notebook. To explore it, please refer to https://medium.com/codait/the-visual-python-debugger-for-jupyter-notebooks-youve-always-wanted-761713babc62."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git and GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the directory that where this jupyter notebook file locates in, init a Git repository.\n",
    "2. Checkout a new branch called dev and commit the current notebook within this branch.\n",
    "3. Merge the dev branch to the master branch (the default branch).\n",
    "4. Create a temporary repository (just for practicing and you can delete it later) in GitHub. \n",
    "5. Push new changes in the master branch to the remote repository created in step 4.\n",
    "6. Checkout the dev branch again and do some changes to your notebook, and then repeat step 3 and step 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ git init    // Initializing git in the working directory\n",
    "\n",
    "$ git checkout -b dev   // new branch 'dev' generated\n",
    "$ git branch            // check the current branch pointed to\n",
    "$ git add filename      // add file to the current git branch\n",
    "$ git commit -a -m 'added file to master'  // commit with a message\n",
    "$ git status            // check the git status\n",
    "\n",
    "$ git checkout -b master  // new branch 'master' created \n",
    "$ git branch              // check the current branch pointed to\n",
    "$ git merge dev           // merging branch 'dev' with master branch\n",
    "\n",
    "\n",
    "Setting up Github account:\n",
    "\n",
    "Go to github.com\n",
    "Create an account/login\n",
    "Click the new repository button in the top-right. You’ll have an option there to initialize the repository with a README file, but I don’t.\n",
    "Click the “Create repository” button.\n",
    "\n",
    "$ git remote add origin git@github.com:username/new_repo   //$ git remote add origin https://github.com/username/new_repo\n",
    "$ git push -u origin master    // pushing code to github repository\n",
    "$ git commit -a -m 'pushed code to repository'\n",
    "\n",
    "$ git checkout dev   // git pointing to 'dev' in local repository\n",
    "$ git branch         // check the current branch pointed to\n",
    "\n",
    "//modify the file by changing something in it\n",
    "$ git add filename\n",
    "$ git commit -a -m 'modified file'\n",
    "$ git merge dev        // merging branch 'dev' with master branch\n",
    "$ git checkout master\n",
    "$ git branch\n",
    "$ git push -u origin master    // pushing code to github repository\n",
    "$ git commit -a -m 'pushed modified code to repository'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.435px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
